(compbio-bias)=
# Bias 
## We want to trust science
[//]: # (TODO: Restructure this section so that the different sources of bias are in a sensible order and that I have proof for each. Rename "why we care" and "we want to trust science")

```{epigraph}
The imposing edifice of science provides a challenging view of what can be achieved by the accumulation of many small efforts in a steady objective and dedicated search for truth.

-- Charles H. Townes
```

[//]: # (TODO: Rewrite this so that the vibe is more "hey we all agree that we want to be able to rely on science and less "FUCK ALL OF YOU")

We want to trust the results of scientific research. 
Not only when it’s our own (because it’s fun and exciting to look for and find the truth), but because science builds on itself and building on shaky ground wastes time and money.
Moreover, scientific research is generally paid for by tax, and the results that are generated by it drive policy, drug treatments, and innovations. 

In all fields, science is a search for the truth. 
And in all fields, there are concerns about what makes bad, unreliable, un-useful, or biased research; what must be done or not done to uphold science’s claim to truth, or at least reliability. 

In contrast to other fields, many bioinformatics datasets have been freely available and accessible on the internet since their inception; in this sense the field is far ahead of others.
The issues which affect the reliability of science in general, however, are likely to be present in computational biology, too.
This could have strong effects on the research that is reliant on these large ontologies and databases. 

(self-correcting-mechanism)=
### Science's self correcting mechanism
Scientific results are often based on statistics, so it’s inevitable that some proportion of published scientific results will not be true. 
This isn’t a problem, as over time, researchers can double-check interesting scientific results, and the literature can be updated to reflect that. 
This is sciences *self-correcting mechanism*. 
If a result can be replicated in a different circumstance by a different person, it reinforces the likelihood that the result is true. 
A replication doesn’t have to reveal the exact same level of statistical significance  or effect size to be successful, but (usually, depending on definitions) just a similar result.

## The reproducibility crisis 

```{epigraph}
In science consensus is irrelevant. What is relevant is reproducible results.

-- Michael Crichton
```

The reproducibility crisis is the realisation that worryingly large proportions of research results do not reproduce.
Replication studies have found that only 11% of cancer research findings{cite}`Begley2012-oc`, 20-25% of drug-target findings{cite}`Begley2012-oc,Prinz2011-rh`, and 39% of psychology findings{cite}`Open_Science_Collaboration2015-vi` reproduced. 
Surveys of researchers across disciplines reveal that more than 70% of scientists say they have failed to reproduce another paper’s result, and over 50% say they have failed to reproduce their own results{cite}`Baker2016-tb`. 
It seems that science’s self-correcting mechanism is not working as intended.

This surprising irreproducibility is thought to be due to a multitude of factors, including poor data management, lack of available materials/details of experiments, publication bias, poor statistical knowledge, and questionable research practices{cite}`Bishop2019-hg`. 

### Null Hypothesis Significance Testing

```{epigraph}
Statistical significance is the least interesting thing about the results. You should describe the results in terms of measures of magnitude –not just, does a treatment affect people, but how much does it affect them.

-- Gene V. Glass
```

[//]: # (TODO: Check maths appears correctly and none missing)
[//]: # (TODO: Aside for Fischer being the worst fucking racist)

```{margin} Fischer racism
:name: fischer-racism
Fischer was a racist, and campaigned for the legalisation of eugenic sterilisation.
```

[//]: # (TODO: Cite alpha cutoff)

Null Hypothesis Significance Testing (NHST) is the most popular method by which scientific hypotheses are tested and reported. 
This reporting usually consists mostly of a p-value, a measure of statistical significance: the likelihood that a false positive at least this extreme could be obtained just by chance. 
The threshold for this, usually denoted by $\alpha$ is most often set to 0.05, as recommended by {ref}`Fischer<fischer-racism>`, however this is not necessarily the most sensible cut-off for science today. 
Despite the dominance of p-values as main or only reported statistic across scientific fields, they do not imply that a result is interesting (the effect might be small or the hypothesis uninteresting), or even that it’s likely to be true. 
Sometimes the p-value is not even reported, but only whether or not it crossed the p<0.05 threshold.

### P-hacking

```{figure} ../images/p_hacking.png
---
height: 220px
name: p_hacking
---
Images that are illustrative of researchers approaches to p-values and p-hacking. The left image is [a popular tweet](https://twitter.com/FaustoBustos/status/1103435523777978368), while the right image is [an xkcd comic](https://xkcd.com/1478/)).
``` 

The pressure on scientists to publish means that researchers may be tempted to (or may accidentally, due to statistical ignorance) employ data-mining tactics in order to harvest significant p-values. 
This practice is known as “p-hacking”, and evidence for its existence can be found in distributions of p-values in scientific literature{cite}`Head2015-ns`, as well as popular culture ({numref}`p_hacking`). 
This can include rerunning analysis with different models/covariates, collecting data until a significant p-value is reached, or performing 20 experiments and only publishing the results of one. 

```{epigraph}
The first principle is that you must not fool yourself – and you are the easiest person to fool. 
-- Richard Feynman 
```

There are several suggested tonics to the problem of uninformative and ubiquitous p-values.
Reporting p-values as numbers rather than in relation to a threshold (e.g. p<0.05) is starting point. 
Information about statistical power and effect size should also be provided. 
In addition to giving researchers reading a paper a better idea of the quality of it, this also allows science to self-correct a little easier, since individual p-values can then be combined into more reliable p-values, using for example Fischer’s method{cite}`Fisher1990-ro`. 

For cases where many hypotheses are being generated at once (for example in GWAS), multiple hypothesis corrections (e.g. the Bonferroni correction{cite}`Dunn1958-sj` or the False Discovery Rate{cite}`Benjamini1995-me`) can be employed to adjust the p-value to account for this.

### Publication bias against negative results
Although with standard p-value and statistical power cut-offs, negative results are more likely to be true than positive ones{cite}`Ioannidis2005-mo`, negative results are much harder to publish. 
This bias is likely to be responsible for the draw of questionable research practices like p-hacking. 
It also means that there is a lot of unpublished, negative results which are likely to be repeated, since there is no way that someone could know it has already been done. 
A highly powered negative result could be very interesting, for example, we know hardly anything about which genes do not appear to affect phenotypes, since these results are not published{cite}`Barbaric2007-zm`, but they would help enormously with the challenge of creating a gold standard data set for gene function prediction.

## Other types of bias
Two other sources of research bias come to mind which may impact the contents of computational biology databases, and therefore methodologies that make use of them.

**1. Replications:** 
Replications are not commonly published (as they are not novel), this discourages people from doing them, or at least from writing them up. 
This is true for both computational methodologies (where often the code and computational environment needed to replicate the research are not provided), and for experimental work.
This means that it is difficult for science to self-correct this work.

**2. Biased study of genes:** 
Some genes are very famous, racking up thousands of publications, while others are entirely unstudied. 
Even looking only at human genes, there is a huge divide between the most and least studied genes. 
This means that there are many functions of genes which will be missing from the gene ontology annotations (for example) for less well-studied genes.

**3. Incentives for speed over quality:** 
In research, there are strong incentives to publish often ("publish or perish").
This incentivises doing the minimum possible to publish, and disincentivises spending time on quality control or providing useful metadata wherever you can publish without it.
There are movements (like the [slow science manifesto](http://slow-science.org/){cite}`noauthor_undated-lr`) to combat this, but it remains the default.
Clearly this impacts on the quality of entries in databases, which is important when these have such strong